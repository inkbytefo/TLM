model:
  vocab_size: 1000
  hidden_dim: 128
  num_layers: 2
  dropout_rate: 0.1

training:
  learning_rate: 1e-3
  batch_size: 32
  num_epochs: 10

data:
  seq_len: 64
